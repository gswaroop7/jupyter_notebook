{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently I have listed pyspark.sql imports, look for pyspark imports as well\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TEST\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"test.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5106782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show();\n",
    "#df.printSchema();\n",
    "#df.columns;df.head(3);\n",
    "#type(df.select(\"Experince\"))\n",
    "#df.select(\"Experince\",\"Name\").show();\n",
    "df.dtypes\n",
    "#df.describe()\n",
    "#df.describe().show()\n",
    "#data2=list(\"Male\") #Will give you ['M', 'a', 'l', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.createDataFrame([(100,\"swaroop\"),(200,\"Pranavi\"),(10,\"Krishna\")],[\"Rank\",\"Name\"])\n",
    "df2.dtypes\n",
    "df2.printSchema()\n",
    "\n",
    "#select is always better to use than withColumn,obvious reason is too many lines for each addition\n",
    "# Also some one pointed DAG will result in toomany shuffle operations if you are using it with window functions. \n",
    "# see https://stackoverflow.com/questions/59789689/spark-dag-differs-with-withcolumn-vs-select \n",
    "\n",
    "# immutable are DataFrames\n",
    "# you can point to previous df column with df.<column_name> or f.col(\"<column_name>\") function as following \n",
    "# lit constant function\n",
    "# \n",
    "df3=df2.select(f.when((df2.Rank == 100) & (df2.Name == \"swaroop\"),f.lit(\"swaroop\"))\\\n",
    "               .otherwise(f.lit(\"not swaroop\"))).alias(\"Random_column\")\n",
    "\n",
    "\n",
    "df4=df2.select(f.col(\"Rank\"),f.col(\"Name\"),f.lit(\"1\").alias(\"What_ever\"),\\\n",
    "               f.when((f.col(\"Rank\") == 100) & (f.col(\"Name\") == \"swaroop\"),\\\n",
    "                    f.lit(\"this is swaroop\")).otherwise(f.lit(\"not swaroop\")).alias(\"Random_column\"))\n",
    "\n",
    "#********** Select will not add to the DF but only selects what is mentioned and puts it as another DF\n",
    "#********** WithColumn will add to the exsisting DF\n",
    "\n",
    "df3.show(truncate=False)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.withColumn(\"screenTime\",f.col(\"Rank\")*-1).show()\n",
    "df4.withColumn(\"screenTime\",df2.Rank*-1).show()   #Catch is that we can use any dataframe df2.rank or df4.rank\n",
    "df4.withColumn(\"newID\",f.col(\"Name\").cast(\"Integer\")).show() #cast to change column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69445f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## select doesnt validate whether the column is already present in that dataframe,hence duplication  ########\n",
    "df5=df4.select(\"*\",(f.col(\"Rank\")*10).alias(\"Rank\"))       ##********************IMP**************###\n",
    "df4.withColumn(\"Rank\",df2.Rank-10).show()       #Value of Rank is changed but no two columns with name \"Rank\"\n",
    "df5.show()                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that there are two columns with same name, with different values and now \\\n",
    "#this dataframe is kind of ambiguous, no further select or drop on these columns can \\\n",
    "#be performed, as it leads to AnalysisException.\n",
    "\n",
    "df5.drop(df5.Rank)        #Will give you Reference 'Rank' is ambiguous, could be: Rank, Rank.\n",
    "df5.select(\"Rank\")        #Will give you Reference 'Rank' is ambiguous, could be: Rank, Rank\n",
    "\n",
    "### bottom line is withColumn = \"duplication check + select\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf96aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 2 different DF\n",
    "        #withColumn or select and add specific columns you want\n",
    "        ## wecan't directly use unionByName or unionAll as we need to allign to have same columns or same no of rows\n",
    "df6=spark.createDataFrame([\\\n",
    "                         Row(Sex=\"M\"),\\\n",
    "                         Row(Sex=\"F\"),\\\n",
    "                         Row(Sex=\"F\")])\n",
    "df2_new = df2.select(\"*\",f.lit(None).alias(\"Sex\"))\n",
    "df6_new = df6.select(\"*\",f.lit(None).alias(\"Name\"),f.lit(None).alias(\"Rank\"))\n",
    "df6_new.unionByName(df2_new).show()     #unionByName will join according to the column name\n",
    "df6_new.unionAll(df2_new).show()        #unionAll will join based on no of rows\n",
    "\n",
    "###Simply add column and update it later with conditions\n",
    "cols = ['Name',\"Rank\",\"Sex\"]    \n",
    "df7=df2.withColumn(\"Sex\", f.lit(None)).select(cols).show()\n",
    "#or\n",
    "df7=df2.withColumn(\"Sex\", f.lit(None)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##join might help here,try when you have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f326b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b70c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To learn based on creating,reading,adding DF\n",
    "    #struct type creating nested or complex columns\n",
    "        #You can use\n",
    "        #schema = StructType([StructField(\"Sex\",StringType(),True)])\n",
    "    #Window function and #partition by column    https://stackoverflow.com/questions/59789689/spark-dag-differs-with-withcolumn-vs-select\n",
    "    #Anything that is opened in tabs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
