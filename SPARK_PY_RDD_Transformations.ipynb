{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd455b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import string as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\",\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2743fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=sc.textFile(\"test2.csv\")\n",
    "file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80316932",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.cache()\n",
    "#file.persist()\n",
    "file.getStorageLevel().useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f61a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.getStorageLevel().useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.map(lambda x: (x.split(\",\")[0],x.split(\",\")[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat=file.flatMap(lambda x: (x.split(\",\")[1],x.split(\",\")[2]))\n",
    "type(flat)\n",
    "len(flat.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59854f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Adding conditions for rdd using filter,\n",
    "file2=sc.textFile(\"test3.csv\")\n",
    "\n",
    "#removing the first line from file as it is header\n",
    "first_line=file2.first()\n",
    "rdd2=file2.filter(lambda x: x != first_line)     #remove header\n",
    "rdd3=rdd2.filter(lambda x: x != \"\")              #remove last empty line\n",
    "#rdd3.collect()\n",
    "\n",
    "#rdd3.map(lambda x: int(x.split(\",\")[2])).collect()\n",
    "rdd3.filter(lambda x: int(x.split(\",\")[2]) > 5000).collect() #gives you enitre x if conditions matches\n",
    "\n",
    "#rdd3.map(lambda x: x[0],filter(lambda y: int(y.split(\",\")[2]) > 5000, rdd3)).collect() # does not work\n",
    "rdd3.map(lambda x:x.split(\",\")[0] if (int(x.split(\",\")[2]) > 5000) else \"\").collect() #doesnot work without else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(100), 4)\n",
    "#rdd.collect()\n",
    "rdd.sample(True, 0.05, 80).collect()  #[11, 33, 33, 42, 74]  33 is repeated becuase of True(withReplacement)\n",
    "rdd.sample(False, 0.05, 80).collect()  #[16, 17, 61, 72, 88] 80 is the starting number(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.intersection(rdd4).collect() #common elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7455e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.union(file2).collect() #add rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcea347",
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55234b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.sortBy(lambda x: int(x.split(\",\")[2]),False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4=rdd3.repartition(3)\n",
    "rdd4.collect()\n",
    "rdd4.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********You have to use lambda function on all operations except mapPartitions, as they iterate themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(partition):\n",
    "        for element in partition:\n",
    "                yield element.split(\",\")[2]\n",
    "rdd4.mapPartitions(fun).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2183b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(index,partition):\n",
    "        salary=[]\n",
    "        for element in partition:\n",
    "            salary.append(element.split(\",\")[2])\n",
    "        yield (index,salary)\n",
    "rdd4.mapPartitionsWithIndex(fun).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c73432",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=rdd4.groupBy(lambda x: x.split(\",\")[0]).collect()  #returns pyspark.resultiterable.ResultIterable since...\n",
    "                                                    # ..we are grouping by name, we get two columns of data\n",
    "\n",
    "#lets see what is there in first element\n",
    "tuple1=list1[0]\n",
    "for x in (tuple1[1]):\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "#else you can also do a map on groupby, so that you directly get a list\n",
    "rdd4.groupBy(lambda x: x.split(\",\")[0]).map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "r=sorted(x.cogroup(y).collect())              #cogroup groups to rdds,no need to have any \n",
    "print (r)\n",
    "for i in r:\n",
    "     for j in i:\n",
    "            if (type(j) == 'str'):\n",
    "                print (j)\n",
    "            else:\n",
    "                for k in j:\n",
    "                    print (list(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4.keyBy(lambda x: int(x.split(\",\")[2])*2).collect() #same as map but with key value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61528955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another complex example of keyby and cogroup together\n",
    "x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
    "y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ddaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd8=file.repartition(3)         #repartition created equal size\n",
    "rdd4.zip(rdd8).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f730a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4.zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coalese will increase or decrease partitions.uses exsisting partition. less shuffle compared \n",
    "#to repartiton and hence fast. Hence unequal partitions.\n",
    "rdd9=rdd4.coalesce(2)\n",
    "rdd9.glom().collect()     #converts everything to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ee4bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba2ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
